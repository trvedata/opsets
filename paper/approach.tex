\section{The OpSets Approach}\label{sec:approach}

At a high level, consistency models for distributed data systems can be classified into two categories:
\begin{description}
\item[Strong consistency models] such as serialisability or linearisability \cite{Herlihy:1990jq} attempt to make a system behave like a single sequentially executing node, even when it is in fact replicated and concurrent.
An unavoidable downside of these models is that they require waiting for synchronous network communication before any operation or transaction is allowed to complete \cite{Davidson:1985hv,Gilbert:2002il}.
Thus, a node cannot make progress while it is offline or partitioned from other nodes.
In a formal sense, enforcing a strong consistency model is equivalent to consensus \cite{Chandra:1996cp,Herlihy:1991gk}, which implies that the algorithm cannot be guaranteed to terminate in an asynchronous system \cite{Fischer:1985tt}.

\item[Weak consistency models] are employed by systems that prioritise availability over strong consistency; for example, systems that require nodes to be able to make progress while offline or while partitioned from other nodes.
In such systems, each node typically reads and manipulates a local copy of the shared state, and propagates any changes to that state asynchronously to other nodes.
Examples of weak consistency models in this category are \emph{causal consistency} \cite{Attiya:2015dm,Mahajan:2011wz,Lloyd:2011hz} and \emph{eventual consistency} \cite{Bailis:2013jc,Burckhardt:2014hy,Terry:1994fp,Vogels:2009ca}.
\end{description}

In this work we focus on the latter category of weak consistency models.
However, as we shall demonstrate shortly, the OpSets approach allows us to specify our consistency model more precisely than existing techiques permit.

\subsection{Eventual Consistency is Insufficient}\label{sec:eventual-consistency}

Eventual consistency is usually informally defined as follows: \emph{if no new updates are made to the shared state, all nodes will eventually have the same data}.
This is a very weak definition for several reasons:
\begin{itemize}
\item The premise, \emph{if no new updates are made}, may never be true (if the shared state is continually modified because the system is never quiescent).
In that case, eventual consistency becomes a vacuous statement.

\item Many trivial algorithms satisfy the requirement of converging towards the same state.
For example, a system could simply discard writes, and thus converge to a state in which operations have been ignored.
Although such a system would not be useful in practice, the definition of eventual consistency does not capture the requirement that writes should be persistent.

\item In a system that allows nodes to make progress while they are partitioned from other nodes, it is inevitable that the local state of individual nodes might diverge due to concurrent modifications.
Such divergent states must then be merged at a later time, when communication between the nodes is restored.
Even if we require that this merge operation does not discard data that has been written, eventual consistency does not specify which states should be considered valid results of the merge operation.
\end{itemize}

\subsection{Introducing OpSets}\label{sec:opsets-intro}

The OpSets approach is a simple abstraction that allows us to be more precise about the consistency properties of a replicated data system.
We assume that the system consists of a set of \emph{nodes} connected by a network.
These nodes concurrently access some \emph{shared data structure}, which may be a relational database (consisting of rows in tables), a text document (an ordered list of characters), a vector-graphics document (a tree of records describing graphical objects), or any other kind of data structure.

We assume that each node has a local copy of the shared data structure, which it can read and modify without locking or any other coordination with other nodes.
Whenever a node makes a modification to that structure, it records the change as an \emph{operation}.
For example, an operation may describe a particular insertion at a particular position in a document.
We assume that each operation has a unique identifier that is different from any other operation generated anywhere in the system.
For example, the identifier may consist of a unique node identifier and a sequence number.

Each node locally maintains a set of operations, the \emph{OpSet}.
Whenever a node makes a change, it adds the corresponding operation to its OpSet, and also broadcasts the operation to other nodes.
Whenever a node receives an operation from another node, that operation is also added to the recipient's local OpSet.
Any operations that are lost in the network are retransmitted as necessary.
Operations remain immutable throughout this process.

Thus, the OpSet is a monotonically growing set of operations, and every operation is eventually contained in the OpSet of every node from which it is not permanently partitioned.
(To deal with unbounded growth, we discuss garbage collection later in this paper.)
Any two communicating nodes can merge their OpSets using the standard set union, which is commutative, associative, and idempotent, ensuring that communicating nodes converge towards the same OpSet contents.

\subsection{Data Structures as Queries}\label{sec:queries}

Existing algorithms for maintaining replicated state, such as CRDTs, describe how a node's local state may be manipulated as a result of operations.
We now depart from this convention and present an alternative formulation of replicated data structures.

In the OpSets approach, we require that the shared data structure is never manipulated directly.
Instead, we assume the existence of a pure function that takes an OpSet as input, and returns the current state of the shared data structure according to the OpSet.
The transformation from OpSet to data structure is deterministic and depends only on the contents of the OpSet -- regardless of whether that data structure is a relational database, a text document, a tree, a graph, or anything else.
All nodes in the system employ the same transformation function.
Consequently, whenever any two nodes have the same OpSet, their view of the shared data structure must also be equal.

One can regard the OpSet of being a \emph{database of facts}, containing all of the changes ever made to the shared data.
The function that deterministically transforms this database into some derived data structure is a \emph{query}.
The resulting data structure is, in database terminology, a \emph{materialized view} onto the underlying set of operations.

We show in the following sections how many practical data structures can be expressed as queries.
For now we note that this construction trivially ensures eventual consistency: as two nodes converge towards the same OpSet contents, any data structure that is deterministically derived from the OpSet must also converge.
By using a deterministic query language to derive the data structure from the OpSet, we can~-- by construction~-- rule out any violations of this convergence property.

Moreover, we can take advantage of a rich body of existing research on query languages and materialized view maintenance.
When a new operation is added to the OpSet, a query execution engine can determine the change to the derived data structure that results from the addition of the new operation.
Determining this change to the query result is known as \emph{incremental view maintenance}, and we build upon extensive prior research in this area.

\subsection{Operation Serializability}\label{sec:op-serial}

Like in the definition of eventual consistency in Section~\ref{sec:eventual-consistency}, deriving a data structure from an OpSet ensures convergence, but it does not define how an operation should take effect.
To refine our correctness properties, we must define the expected semantics of operations.

Normally, reasoning about the semantics of concurrently modified data structures is difficult and error-prone.
However, the OpSets model enables us to reason about operation semantics in a sequential way, and directly extend the sequential semantics to arbitrary concurrent executions.

We previously required that every operation has a unique identifier.
We now assume that we also have a total ordering on operation identifiers, and that this total order is a linear extension of the partial order that captures their causal dependencies (the \emph{happens-before} relation).
That is, whenever a node generates a new operation, the identifier of the new operation must be greater than that of any existing operation in the OpSet of the node generating the new operation.
This requirement can easily be met by using Lamport timestamps \cite{Lamport:1978jq} as identifiers.

Now observe that for any OpSet there exists a unique sequence of operations, containing all operations of the OpSet in ascending order of their identifier.
We can specify the semantics of each operation~-- that is, the effect of the operation on the query result~-- when applied in this sequential order.
Since we know that the query result is determined entirely by the OpSet, we know that even if the operations arrive at a node in any arbitrary order, the final query result must be the same as if they had been applied sequentially in order of ascending identifier.

In other words, we have \emph{operation serializability}: the data structure derived from an OpSet is equal to the outcome of applying the operations in their serial order.
It is therefore sufficient for us to define the semantics of each operation under serial execution, and we know that this definition will also define its semantics in arbitrary concurrent executions: any state in a concurrent execution corresponds to some OpSet, and the operations in that OpSet are serializable.
